{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtaLPWSdhSQX"
   },
   "source": [
    "\n",
    "Audio Classifier Tutorial (Fast.ai)\n",
    "=========================\n",
    "**Author**: `Bruno Lima <https://github.com/limazix>`\n",
    "\n",
    "This is the recreation of the PyTorch tutorial for audio classification using the Fastai framework.\n",
    "- [torchaudio](https://github.com/pytorch/audio)\n",
    "- [fastai](https://docs.fast.ai/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First, letâ€™s import the common packages ``pandas`` and ``numpy``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8KGhRRshSQR"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nF73jgL2hSQk"
   },
   "source": [
    "Importing the Dataset\n",
    "---------------------\n",
    "\n",
    "We will use the UrbanSound8K dataset to train our network. It is\n",
    "available for free `here <https://urbansounddataset.weebly.com/>`_ and contains\n",
    "10 audio classes with over 8000 audio samples! Once you have downloaded\n",
    "the compressed dataset, extract it to your current working directory.\n",
    "First, we will look at the csv file that provides information about the\n",
    "individual sound files. ``pandas`` allows us to open the csv file and\n",
    "use ``.iloc()`` to access the data within it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the download at the data folder, let's map the data paths into global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR=os.path.normpath(os.path.join(os.getcwd(), 'data/UrbanSound8k'))\n",
    "DATA_META_FILE=os.path.join(DATA_ROOT_DIR, 'metadata/UrbanSound8k.csv')\n",
    "DATA_AUDIO_DIR=os.path.join(DATA_ROOT_DIR, 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8btdLhPdhSQl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice_file_name    100032-3-0-0.wav\n",
      "fsID                         100032\n",
      "start                             0\n",
      "end                        0.317551\n",
      "salience                          1\n",
      "fold                              5\n",
      "classID                           3\n",
      "class                      dog_bark\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_meta = pd.read_csv(DATA_META_FILE)\n",
    "print(data_meta.iloc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7miODyWlhSQp"
   },
   "source": [
    "The 10 audio classes in the UrbanSound8K dataset are: `air_conditioner`, `car_horn`, `children_playing`, `dog_bark`, `drilling`, `enginge_idling`, `gun_shot`, `jackhammer`, `siren`, and `street_music`.\n",
    "\n",
    "Each class has its own folder and id. For instance, the last print shows that the audio file has:\n",
    "- **class** - dog_bark\n",
    "- **id** - 3\n",
    "- **folder** - 5\n",
    "\n",
    "We will need the classID and the full file path only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8732 entries, 0 to 8731\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   path    8732 non-null   object\n",
      " 1   label   8732 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 136.6+ KB\n"
     ]
    }
   ],
   "source": [
    "def build_file_path(item):\n",
    "    item['path'] = 'fold{}/{}'.format(item['fold'], item['slice_file_name'])\n",
    "    item['label'] = item['class']\n",
    "    return item[['path', 'label']]\n",
    "\n",
    "data = data_meta.apply(build_file_path, axis=1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast.ai Data Structure\n",
    "\n",
    "In order to run DNN with Fast.ai, it's necessary to create a DataBunch, the basic framework data structure.\n",
    "\n",
    "The Fast.ai has a few built-in DataBanch, such as ImageDataBunch for image processing and TextDataBunch for NLP, but none for audio processing yet. Therefore, we'll have to create a custom one.\n",
    "\n",
    "The simplest way to do that is using the ItemList and ItemBase classes. The first one creates a structure that allows operations to manipulate the dataset, while the second one defines the operation bahavior for one single item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from fastai.data_block import ItemList, ItemBase\n",
    "\n",
    "class AudioItem(ItemBase):\n",
    "\n",
    "    def apply_tfms(self, tfms):\n",
    "        if not tfms: return self\n",
    "        \n",
    "        for t in tfms:\n",
    "            self.data = t(self.data)\n",
    "        return self\n",
    "        \n",
    "class AudioItemList(ItemList):\n",
    "    \n",
    "    def get(self, i):\n",
    "        fn = super().get(i)\n",
    "        file_path = os.path.join(self.path, fn)\n",
    "        return self.open(file_path)\n",
    "\n",
    "    def open(self, fn):\n",
    "        audio, sample_rate = torchaudio.load(fn)\n",
    "        return AudioItem(audio[0, :].view(1, -1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep it simple, we create two methods for the custom ItemList:\n",
    "- **get** - Method to retrieve one single item from the dataset given its position;\n",
    "- **open** - Method used to load the audio file and build the AudioItem object.\n",
    "\n",
    "And one for the custom ItemBase:\n",
    "- **apply_ftms** - Method used to apply transformations in each data from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "\n",
    "Like any machine learning system, it's necessary to perform a few preprocessing steps on the data in order to run the intent algorithm. The original tutorial perform three transformations:\n",
    "- Resample - It perform a downsample from 44.1KHz to 8KHz\n",
    "- MaxClips - It uses a fixed sample size of 160000 clips by completting with zeros or remove when necessary\n",
    "- ClipFrequency - It group every 5th clip to the final sample and ignore de rest\n",
    "\n",
    "For the first transformation, the `torchaudio` library already have an implementation. But not for the last two, so we need to create then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MaxClips(object):\n",
    "    \n",
    "    def __init__(self, max_clips):\n",
    "        self.max_clips = max_clips\n",
    "\n",
    "    def __call__(self, sound_base):\n",
    "\n",
    "        sound_base = sound_base.long()\n",
    "\n",
    "        #tempData accounts for audio clips that are too short\n",
    "        sound = torch.zeros([self.max_clips, 1])\n",
    "        if sound_base.numel() < self.max_clips:\n",
    "            sound[:sound_base.numel()] = sound_base.view(-1, 1)[:]\n",
    "        else:\n",
    "            sound[:] = sound_base.view(-1, 1)[:self.max_clips]\n",
    "\n",
    "        return sound\n",
    "\n",
    "class ClipFrequency(object):\n",
    "    \n",
    "    def __init__(self, size, frequency):\n",
    "        self.frequency = frequency\n",
    "        self.size = size\n",
    "        \n",
    "    def __call__(self, sound_base):\n",
    "        sound = torch.zeros([self.size, 1])\n",
    "        #take every fifth sample of soundData\n",
    "        sound[:self.size] = sound_base[::self.frequency]\n",
    "        sound = sound.permute(1, 0)\n",
    "        \n",
    "        return sound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create the transformation pipelie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Resample(), <__main__.MaxClips object at 0x12e1e2ef0>, <__main__.ClipFrequency object at 0x12e1e28d0>]\n"
     ]
    }
   ],
   "source": [
    "from torchaudio.transforms import Resample\n",
    "\n",
    "transforms = [\n",
    "    Resample(orig_freq=44100, new_freq=8000),\n",
    "    MaxClips(max_clips=160000),\n",
    "    ClipFrequency(size=32000, frequency=5)\n",
    "]\n",
    "print(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataBunch Creation\n",
    "\n",
    "We already have the minimun necessary to create the DataBunch. Here, we perform five operations:\n",
    "- **from_df** - It transform the DataFrame with two columns into an AudioItemList instance;\n",
    "- **split_subsets** - It splits the dataset in two, train and validation dataset, based on the given proportions;\n",
    "- **label_from_df** - It informs the ItemList to use the column `label` from the dataset as the item class;\n",
    "- **transform** - It applys the transformation pipeline to bouth datasets, train and validation;\n",
    "- **databunch** - It creates the databunch with a batch size of 128 inputs per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bunch = (AudioItemList.from_df(data, path=DATA_AUDIO_DIR)\n",
    "              .split_subsets(train_size=0.8, valid_size=0.2)\n",
    "              .label_from_df(cols='label')\n",
    "              .transform((transforms, transforms))\n",
    "              .databunch(bs=128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The tutorial built de model class after the M5 CNN architecture for audio processing and classification as presented by [Wei Dai Et al.](https://arxiv.org/pdf/1610.00087.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.functional import F\n",
    "\n",
    "class M5Model(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(M5Model, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(256, 512, 3)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        #input should be 512x30 so this outputs a 512x1\n",
    "        self.avgPool = nn.AvgPool1d(30)\n",
    "        self.fc1 = nn.Linear(512, n_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.avgPool(x)\n",
    "        #change the 512x1 to 1x512\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "After defined the `databunch` and the `model`, it is time to create the main responsible by the larning operations, the `Learner`.\n",
    "\n",
    "Usually, with `pytorch`, that's the moment to define the `optimizer`, the `learning rate`(lr), the `loss function` and methods for training and testing for each epoch. All those things are define manually.\n",
    "\n",
    "With time, it becomes clear that not everything has to be custom made for each experiment scenario. The [Learner](https://docs.fast.ai/basic_train.html#Learner) class provide all those setups already built-in and it uses the the concept of partial to allow a few enhancements.\n",
    "\n",
    "Tho original tutorial uses the `Adan` optimizer with lr `0.1`. The `Learner` use the same optimizer by default but, if experiment request a different one, it can be changed by using the `opt_func` parament from the constructor. Likewise, the `loss function` with the parameter `loss_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import EarlyStoppingCallback, ReduceLROnPlateauCallback\n",
    "from fastai.metrics import accuracy, partial, error_rate\n",
    "from fastai.basic_data import DataBunch\n",
    "from fastai.basic_train import Learner\n",
    "\n",
    "model = M5Model(n_classes=data_bunch.c)\n",
    "\n",
    "learn = Learner(data_bunch, model, metrics=[error_rate, accuracy],\n",
    "               callback_fns=[\n",
    "                   partial(EarlyStoppingCallback, monitor='accuracy', patience=10, min_delta=5e-4),\n",
    "                   partial(ReduceLROnPlateauCallback, monitor='accuracy', patience=5, factor=0.2, min_delta=0)\n",
    "               ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to notice that the above setup is using `error_rate` and `accuracy` as metrics to evaluate the performace of the model while it performs the training and validation phases.\n",
    "\n",
    "In adition, there are two partials:\n",
    "- **EarlyStoppingCallback** - It stops the trainig if a given metric, in this case `accuracy`, does not increases `5e-4` in `10` epochs\n",
    "- **ReduceLROnPlateauCallback** - It changes the lr by `0.2` times if the `accuracy` does not increases in `5` epochs\n",
    "\n",
    "Those two partials avoid overtrainig and helps to scape from a possible plateau.\n",
    "\n",
    "To run the training and the validation phases, the `Learner` has a method called `fit`. In the follow example, it receives two parameters: number of epochs and lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='17' class='' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      42.50% [17/40 4:22:01<5:54:30]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.266597</td>\n",
       "      <td>2.250529</td>\n",
       "      <td>0.883161</td>\n",
       "      <td>0.116838</td>\n",
       "      <td>16:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.236449</td>\n",
       "      <td>2.237231</td>\n",
       "      <td>0.881443</td>\n",
       "      <td>0.118557</td>\n",
       "      <td>17:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.216365</td>\n",
       "      <td>2.202770</td>\n",
       "      <td>0.852806</td>\n",
       "      <td>0.147194</td>\n",
       "      <td>17:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.204523</td>\n",
       "      <td>2.206270</td>\n",
       "      <td>0.856243</td>\n",
       "      <td>0.143757</td>\n",
       "      <td>15:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.188130</td>\n",
       "      <td>2.197274</td>\n",
       "      <td>0.853952</td>\n",
       "      <td>0.146048</td>\n",
       "      <td>15:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.190277</td>\n",
       "      <td>2.196049</td>\n",
       "      <td>0.859679</td>\n",
       "      <td>0.140321</td>\n",
       "      <td>15:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.181231</td>\n",
       "      <td>2.193279</td>\n",
       "      <td>0.845361</td>\n",
       "      <td>0.154639</td>\n",
       "      <td>17:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.175509</td>\n",
       "      <td>2.184804</td>\n",
       "      <td>0.865407</td>\n",
       "      <td>0.134593</td>\n",
       "      <td>17:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.169506</td>\n",
       "      <td>2.176600</td>\n",
       "      <td>0.857961</td>\n",
       "      <td>0.142039</td>\n",
       "      <td>13:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.169297</td>\n",
       "      <td>2.192725</td>\n",
       "      <td>0.870561</td>\n",
       "      <td>0.129439</td>\n",
       "      <td>14:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.163909</td>\n",
       "      <td>2.204478</td>\n",
       "      <td>0.864261</td>\n",
       "      <td>0.135739</td>\n",
       "      <td>17:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.163980</td>\n",
       "      <td>2.189069</td>\n",
       "      <td>0.857961</td>\n",
       "      <td>0.142039</td>\n",
       "      <td>14:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.159013</td>\n",
       "      <td>2.174106</td>\n",
       "      <td>0.860252</td>\n",
       "      <td>0.139748</td>\n",
       "      <td>13:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.150791</td>\n",
       "      <td>2.171727</td>\n",
       "      <td>0.860825</td>\n",
       "      <td>0.139175</td>\n",
       "      <td>13:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.145757</td>\n",
       "      <td>2.174139</td>\n",
       "      <td>0.859679</td>\n",
       "      <td>0.140321</td>\n",
       "      <td>13:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.141451</td>\n",
       "      <td>2.177237</td>\n",
       "      <td>0.861398</td>\n",
       "      <td>0.138603</td>\n",
       "      <td>13:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.139395</td>\n",
       "      <td>2.185987</td>\n",
       "      <td>0.854525</td>\n",
       "      <td>0.145475</td>\n",
       "      <td>13:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='14' class='' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [14/14 01:19<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: reducing lr to 0.002\n",
      "Epoch 17: early stopping\n"
     ]
    }
   ],
   "source": [
    "learn.fit(40, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Considerations\n",
    "\n",
    "It shows how simplier and cleaner is to wright using fast.ai even when no previous module exists, such as `vision` or `text`. \n",
    "\n",
    "Now, it is possible to dig a little deeper on audio processing universe using fast.ai. For the next article, it'll try to improve the accuracy of the model used here by playing with the input data in the preprocessing phase and with the parameters in the training phase.\n",
    "\n",
    "The notebook used in this tutorial is available [here](https://github.com/limazix/audio-processing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
